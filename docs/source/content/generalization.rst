.. _generalization-introduction:

Splam generalizes on non-human species
=========================================================================

If you are considering improving your non-human RNA-Seq alignment files or transcript assemblies and wondering whether Splam is the right tool to use, this page provides the answer!

In short, the answer is **Yes âœ…**!

You can perform the same analysis as described in the :ref:`alignment-detailed-section` and :ref:`annotation-detailed-section` pages. All you need to prepare are the **(1) genome**, and **(2) alignment files or annotation files** of the species you are interested in.

Below is a simple example to show :ref:`Splam works on mouse <_example-of-running-splam-on-mouse>` and a detailed explanation on :ref:`how well Splam works on non-human species <_splam-generalization-performance>`. 

|


.. _example-of-running-splam-on-mouse:

Example: Running Splam on house mouse (*Mus musculus*) 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

In this example, we will be scoring the GRCm39 full assembly of mouse chromosome 19. The steps are very similar to the :ref:`annotation-detailed-section` example, which you should check out first. The key difference is that we will be running Splam with an extra file argument (the assembly report) that is needed for non-human species. Also, in this genome, we are using RefSeq identifiers. 

|

.. _mouse-prepare-input:

Step 1: Prepare input files
++++++++++++++++++++++++++++++

There are three required files, which are the same as in the :ref:`human annotation file evaluation <annotation-prepare-input>`. The fourth file is required for **non-human species**. It is the assembly report, which contains chromosome identifiers and sizes. This information is built into Splam for humans, but not for other species. You should download the assembly report from the same source as your :code:`GFF` or :code:`GTF` annotation file and :code:`FASTA` genome. 

.. admonition:: Input files
   :class: note

   1. An annotation file in :code:`GFF` or :code:`GTF` format [`example file: mouse_chr19.gff <https://github.com/Kuanhao-Chao/splam/blob/main/test/mouse_chr19.gff>`_].  
   2. A reference genome in :code:`FASTA` format [`example file: mouse_chr19.fa <https://github.com/Kuanhao-Chao/splam/blob/main/test/mouse_chr19.fa>`_].
   3. The Splam model, which you can find here: `splam.pt <https://github.com/Kuanhao-Chao/splam/blob/main/model/splam_script.pt>`_
   4. An assembly report in :code:`txt` format [`example file: GRCm39_assembly_report.txt <https://github.com/Kuanhao-Chao/splam/blob/main/test/GRCm39_assembly_report.txt>`_].

|

.. admonition:: Assembly report
   :class: important

   For non-human species, remember to include an assembly report with your input files! 

|



.. _splam-generalization-performance:

Performance on non-human species
++++++++++++++++++++++++++++++++++++++++++++

Although Splam was trained on human datasets, we have shown that the model is able to generalize to other eukaryotic genomes, including plants. In our investigation, we tested the performance of Splam on the Chimpanzee (*Pan troglodytes*), Mouse (*Mus musculus*), and Thale cress, a flowering plant (*Arabidopsis thaliana*), genomes. We report that Splam outperforms SpliceAI in recall, precision, and accuracy, on every score threshold for every species. 

.. _generalization-data-curation:

Data curation 
----------------

For each species, we curated our positive and negative datasets similar to the Positive-MANE and Negative-Random strategies from :ref:`Behind the Scenes <data-curation>`. The positive dataset was generated from the complete RefSeq/TAIR annotation files downloaded from NCBI, where we extracted the introns from every transcript, filtered out poor annotations, and saved the remaining ~200k as splice junctions. The negative dataset was generated by randomly selecting variable-length canonical GT-AG pairs from the opposite strand of protein-coding genes. 

For both datasets, we randomly sampled 25,000 splice junctions and ran Splam and SpliceAI on them to compare results.


|

.. _generalization-key-findings:

Key findings
----------------

We find that Splam is able to quickly and accurately classify positive and negative samples, even in species as distant as plants (*Arabidopsis thaliana*). For the positive samples, Splam correctly classified the vast majority with a high score, whereas SpliceAI struggled more, particularly with *Arabidopsis*. Moreover, we find that Splam is decisive, with medians values for all three species at 1.0, while SpliceAI's distribution is more evenly spread (:numref:`generalization-score-dist`). 

.. _generalization-score-dist:
.. figure::  ../_images/generalization_pos_score_dist.png
   :align:   center
   :scale:   8 %

   A comparison of the score distributions between Splam (green) and SpliceAI (orange) for the positive dataset. The top row represents donor site scores and the bottom row represents acceptor site scores. The darkened vertical line through the distribution represents the median value, while the two dotted vertical lines represent the first and third quartiles. Splam demonstrates the ability to correctly score positive samples highly, with a peak near 1.0 for all three species. SpliceAI's distributions are more spread out, especially for *Arabidopsis*, which exhibits an M-shaped distribution.

|

Combining this result with the negative data, we calculate summary statistics that help us gauge the performance of Splam at various thresholds. We observe that the recall/sensitivity, precision, and accuracy of Splam outperform SpliceAI at every score threshold for every species, visualized in :numref:`generalization-heatmap`. Additionally, the figure demonstrates Splam's consistency across a wide range of score thresholds. 

.. _generalization-heatmap:
.. figure::  ../_images/generalization_performance_heatmap.png
   :align:   center
   :scale:   22 %

   A grid of heatmaps portraying the recall (top), precision (middle), and accuracy (bottom) of both models on the chimpanzee (left), mouse (middle), and *Arabidopsis* (right) genomes, across a variety of score thresholds (x-axis). For each heatmap, the top three rows (green) represent Splam, and the bottom three rows (orange) represent SpliceAI, where each is examined at the donor, acceptor, and splice junction levels. The performance metrics are reported as percentages on the heatmap. We observe that Splam exhibits consistently high recall and accuracy whereas SpliceAI's swiftly declines with increasing score thresholds. Precision for both models is consistently high. 

|

The results of this investigation demonstrate Splam's ability to generalize well on non-human species, including both animal and plant genomes. Additionally, we show Splam performing consistently and accurately on a wide range of score thresholds. 

|
|
|
|
|

.. image:: ../_images/jhu-logo-dark.png
   :alt: My Logo
   :class: logo, header-image only-light
   :align: center

.. image:: ../_images/jhu-logo-white.png
   :alt: My Logo
   :class: logo, header-image only-dark
   :align: center